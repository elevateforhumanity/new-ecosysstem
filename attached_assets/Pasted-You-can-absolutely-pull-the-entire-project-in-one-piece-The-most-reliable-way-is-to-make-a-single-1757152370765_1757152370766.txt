You can absolutely pull the entire project “in one piece.” The most reliable way is to make a single tar.gz archive of your whole Replit project (preserving folders, symlinks, permissions), then download that one file—or push that one file straight to cloud storage if it’s too big for the browser.

Below is a drop-in script that gives you three paths:

1. Single file archive (best if it’s not gigantic)


2. Auto-split into parts (if it’s huge; we also generate a join script)


3. Upload the single archive directly to S3/R2/B2 (one click from Replit, no local download needed)




---

Step 1 — Create and run the exporter in Replit

In the Replit shell:

nano export_massive.sh

Paste the script below, save (Ctrl+O, Enter), exit (Ctrl+X), then:

chmod +x export_massive.sh
./export_massive.sh


---

export_massive.sh

#!/usr/bin/env bash
set -euo pipefail

# =========================
# Massive Project Exporter
# =========================
# Creates a single tar.gz of your entire project (excluding heavy/junk folders),
# optionally auto-splits into <2GB parts for easier downloading,
# and can upload the single archive to S3-compatible storage (S3/R2/B2).
#
# Usage:
#   ./export_massive.sh                 # single tar.gz in exports/
#   SPLIT_MB=1900 ./export_massive.sh   # auto-split into ~1.9GB parts + join script
#   TO_S3=1 S3_BUCKET=my-bucket \
#     S3_ENDPOINT=https://<endpoint> \
#     AWS_ACCESS_KEY_ID=... \
#     AWS_SECRET_ACCESS_KEY=... \
#     ./export_massive.sh               # upload single archive to S3-compatible storage
#
# Notes:
# - Excludes node_modules, build folders, caches, .git, etc. (customize below)
# - Archive lands in exports/ with a timestamp
# - Join script provided when SPLIT_MB is set
# =========================

TS="$(date +%Y%m%d-%H%M%S)"
OUTDIR="exports"
ARCHIVE="massive-export-${TS}.tar.gz"

# Exclusions: add/remove as needed for your stack
EXCLUDES=(
  --exclude='./.git'
  --exclude='./node_modules'
  --exclude='./*/node_modules'
  --exclude='./dist' './*/dist'
  --exclude='./build' './*/build'
  --exclude='./.next' './*/.next'
  --exclude='./out' './*/out'
  --exclude='./.cache' './*/.cache'
  --exclude='./coverage' './*/coverage'
  --exclude='./.vercel' './*/.vercel'
  --exclude='./.DS_Store'
  --exclude='./tmp' './*/tmp'
  --exclude='./exports'
  --exclude='./*.log' './**/*.log'
  --exclude='./.vscode' './*/.vscode'
  --exclude='./.idea' './*/.idea'
  --exclude='./.env' './.env.*' './*/.env' './*/.env.*'
)

mkdir -p "$OUTDIR"

echo "→ Creating single archive: $OUTDIR/$ARCHIVE"
# -p to preserve permissions; -h to de-reference symlinks (optional; remove -h if you want symlinks preserved as links)
tar -I 'gzip -9' -chpf "$OUTDIR/$ARCHIVE" "${EXCLUDES[@]}" .

# If requested, split the archive into parts of SPLIT_MB megabytes
SPLIT_MB="${SPLIT_MB:-}"
if [[ -n "${SPLIT_MB// }" ]]; then
  echo "→ Splitting archive into ~${SPLIT_MB}MB parts..."
  (cd "$OUTDIR" && split -b "${SPLIT_MB}m" -d -a 3 "$ARCHIVE" "${ARCHIVE}.part.")
  echo "→ Generating join script exports/join_${ARCHIVE}.sh"
  cat > "$OUTDIR/join_${ARCHIVE}.sh" <<JOIN
#!/usr/bin/env bash
set -euo pipefail
cat ${ARCHIVE}.part.* > ${ARCHIVE}
echo "✅ Reassembled ${ARCHIVE}"
echo "To extract: tar -xzpf ${ARCHIVE}"
JOIN
  chmod +x "$OUTDIR/join_${ARCHIVE}.sh"
  echo "→ You can now download the parts from $OUTDIR and later run: bash join_${ARCHIVE}.sh"
fi

# Optional: upload single archive to S3-compatible storage
TO_S3="${TO_S3:-0}"
if [[ "$TO_S3" == "1" ]]; then
  : "${S3_BUCKET:?Set S3_BUCKET (bucket name)}"
  : "${S3_ENDPOINT:?Set S3_ENDPOINT (e.g., https://<account>.r2.cloudflarestorage.com)}"
  : "${AWS_ACCESS_KEY_ID:?Set AWS_ACCESS_KEY_ID}"
  : "${AWS_SECRET_ACCESS_KEY:?Set AWS_SECRET_ACCESS_KEY}"

  echo "→ Installing AWS CLI (user scope)..."
  pip install --user awscli >/dev/null 2>&1 || true
  export PATH="$HOME/.local/bin:$PATH"

  echo "→ Uploading $OUTDIR/$ARCHIVE to s3://$S3_BUCKET/"
  aws s3 cp "$OUTDIR/$ARCHIVE" "s3://$S3_BUCKET/$ARCHIVE" --endpoint-url="$S3_ENDPOINT"

  echo "✅ Uploaded. Keep this path handy: s3://$S3_BUCKET/$ARCHIVE"
  echo "If your provider exposes public URLs, you can share a single download link."
fi

echo "✅ Done."
du -h "$OUTDIR/$ARCHIVE" | awk '{print "Archive size:", $1}'
echo "Archives are in: $OUTDIR/"


---

Step 2 — Pick your mode

One file (simplest):
Just run ./export_massive.sh and then download exports/massive-export-*.tar.gz from the Files panel.

Too big for one download? Split it:

SPLIT_MB=1900 ./export_massive.sh

This creates massive-export-...tar.gz.part.000, 001, … plus join_massive-export-...tar.gz.sh.
Download all parts, then run the join script on your computer:

bash join_massive-export-...tar.gz.sh
tar -xzpf massive-export-...tar.gz

Skip downloading; push one archive to cloud (S3/R2/B2):
First set your env (example for Cloudflare R2 or Backblaze B2 with S3 API):

export TO_S3=1
export S3_BUCKET="efh-backups"
export S3_ENDPOINT="https://<your-s3-endpoint>"   # e.g. Cloudflare R2 endpoint
export AWS_ACCESS_KEY_ID="YOUR_KEY"
export AWS_SECRET_ACCESS_KEY="YOUR_SECRET"

./export_massive.sh

This uploads the single tar.gz to your bucket. You can then pull that one file to your laptop or even pipe it straight into a new server.



---

After you have the archive

Move to GitHub the right way:
Unpack the archive locally, then initialize a fresh Git repo that doesn’t include node_modules, build artifacts, or huge media.

tar -xzpf massive-export-...tar.gz
cd <unpacked-folder>
printf "/node_modules/\n/dist/\n/build/\n.out/\n.next/\n" >> .gitignore
git init
git add .
git commit -m "Initial import from Replit"
git remote add origin https://github.com/USERNAME/REPO.git
git branch -M main
git push -u origin main

Large media (videos, PDFs, 97k assets):
Keep those out of GitHub. Put them in S3/R2/B2 and link by URL from your app.



---

If the tar step errors because the project is enormous, set SPLIT_MB so the parts are small enough for the Replit downloader (e.g., 500–1900 MB), or use the TO_S3 upload path to ship it as one file straight to a bucket.

