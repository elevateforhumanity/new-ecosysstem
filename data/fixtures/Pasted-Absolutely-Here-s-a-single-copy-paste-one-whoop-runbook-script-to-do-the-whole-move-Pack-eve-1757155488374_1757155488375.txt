Absolutely. Here’s a single, copy-paste “one-whoop” runbook + script to do the whole move:

Pack everything on Replit

Upload archives to Cloudflare R2

Push code-only to GitHub (for Vercel)

Leave assets ready in R2

Print the exact DNS you’ll set for Vercel and where to grab the Durable CNAME



---

0) What you need ready

Cloudflare R2

Bucket: efh-archive

Account ID: (you have it onscreen)

Access Key ID / Secret Access Key (S3-compatible)

Endpoint: https://<ACCOUNT_ID>.r2.cloudflarestorage.com


GitHub

An empty repo created (example: https://github.com/elevateforhumanity/efh-platform.git)

A Personal Access Token (classic, repo permissions) or you’ll push over HTTPS with normal login


Vercel

Project already created (connected to GitHub)


Durable

Site created; you’ll copy the Durable DNS target from its dashboard (a CNAME they show)




---

1) Set environment variables in Replit (once)

Replace the placeholders, then paste the block into the Replit shell:

# --- R2 (Cloudflare) ---
export S3_BUCKET="efh-archive"
export S3_ENDPOINT="https://<ACCOUNT_ID>.r2.cloudflarestorage.com"
export AWS_ACCESS_KEY_ID="<YOUR_R2_ACCESS_KEY_ID>"
export AWS_SECRET_ACCESS_KEY="<YOUR_R2_SECRET_ACCESS_KEY>"
export AWS_DEFAULT_REGION="auto"

# --- GitHub ---
export GIT_REMOTE="https://github.com/<USER>/<REPO>.git"   # e.g. https://github.com/elevateforhumanity/efh-platform.git
# If you prefer PAT auth (non-interactive), use: https://<USER>:<TOKEN>@github.com/<USER>/<REPO>.git

# --- Project + Domains (for the summary) ---
export PROJECT_NAME="efh-platform"
export ROOT_DOMAIN="elevateforhumanity.org"
export APP_SUBDOMAIN="app"    # will be app.elevateforhumanity.org for Vercel
export ASSETS_PREFIX="assets"  # optional if later you map R2 behind a subdomain


---

2) Create the all-in-one script on Replit

In the shell:

nano efh_migrate.sh

Paste this entire script, save (Ctrl+O, Enter), exit (Ctrl+X), then run it.

#!/usr/bin/env bash
set -euo pipefail

# ===== Helpers =====
say(){ printf "\n%s\n" "$*"; }
need(){ command -v "$1" >/dev/null 2>&1 || { echo "Missing $1"; exit 1; }; }

TS="$(date +%Y%m%d-%H%M%S)"
OUTDIR="exports"
CODE_TAR="$OUTDIR/code-only-$TS.tar.gz"
ASSET_TAR="$OUTDIR/assets-only-$TS.tar.gz"

mkdir -p "$OUTDIR"

# ===== Check env =====
: "${S3_BUCKET:?Set S3_BUCKET}"
: "${S3_ENDPOINT:?Set S3_ENDPOINT}"
: "${AWS_ACCESS_KEY_ID:?Set AWS_ACCESS_KEY_ID}"
: "${AWS_SECRET_ACCESS_KEY:?Set AWS_SECRET_ACCESS_KEY}"
: "${GIT_REMOTE:?Set GIT_REMOTE}"
PROJECT_NAME="${PROJECT_NAME:-repo}"
ROOT_DOMAIN="${ROOT_DOMAIN:-example.com}"
APP_SUBDOMAIN="${APP_SUBDOMAIN:-app}"
ASSETS_PREFIX="${ASSETS_PREFIX:-assets}"

# ===== Tools =====
if ! command -v aws >/dev/null 2>&1; then
  say "Installing AWS CLI (user scope)..."
  pip install --user awscli >/dev/null 2>&1 || true
  export PATH="$HOME/.local/bin:$PATH"
fi
need tar
need gzip
need git

# ===== Build CODE archive (safe for GitHub/Vercel) =====
say "→ Creating CODE archive (excludes node_modules, dist, build, caches, .git, assets folders)"
tar -I 'gzip -9' -chpf "$CODE_TAR" \
  --exclude='./.git' \
  --exclude='./node_modules' --exclude='./*/node_modules' \
  --exclude='./dist' --exclude='./*/dist' \
  --exclude='./build' --exclude='./*/build' \
  --exclude='./.next' --exclude='./*/.next' \
  --exclude='./out' --exclude='./*/out' \
  --exclude='./.cache' --exclude='./*/.cache' \
  --exclude='./coverage' --exclude='./*/coverage' \
  --exclude='./.vercel' --exclude='./*/.vercel' \
  --exclude='./tmp' --exclude='./*/tmp' \
  --exclude='./exports' \
  --exclude='./assets' --exclude='./static' --exclude='./public/assets' \
  --exclude='*.log' \
  .

# ===== Build ASSETS archive (best-effort; ignore if none) =====
say "→ Creating ASSETS archive (assets/, static/, public/assets/ if present)"
tar -I 'gzip -9' -chpf "$ASSET_TAR" \
  ./assets ./static ./public/assets 2>/dev/null || true

say "→ Archives created:"
du -h "$CODE_TAR" 2>/dev/null || true
du -h "$ASSET_TAR" 2>/dev/null || true

# ===== Upload archives to R2 =====
say "→ Uploading archives to R2 bucket: s3://$S3_BUCKET"
aws s3 cp "$CODE_TAR"  "s3://$S3_BUCKET/" --endpoint-url "$S3_ENDPOINT"
[ -f "$ASSET_TAR" ] && aws s3 cp "$ASSET_TAR" "s3://$S3_BUCKET/" --endpoint-url "$S3_ENDPOINT" || true

# ===== Prepare CLEAN Git repo (code only) =====
say "→ Preparing clean Git repo (code only)"
# Unpack code archive into a temp folder to guarantee no heavy junk enters Git
WORKDIR=".export_code_$TS"
mkdir -p "$WORKDIR"
tar -xzf "$CODE_TAR" -C "$WORKDIR"

# Create .gitignore
cat > "$WORKDIR/.gitignore" <<'IGN'
node_modules/
dist/
build/
.next/
out/
.cache/
coverage/
*.log
.env
.env.*
exports/
tmp/
assets/
static/
public/assets/
IGN

# Init repo + push
( cd "$WORKDIR"
  if [ ! -d .git ]; then
    git init
  fi
  git add .
  git commit -m "Initial commit: code-only export ($TS)" || true
  git branch -M main
  # If remote exists, reset; otherwise add
  if git remote | grep -q origin; then
    git remote set-url origin "$GIT_REMOTE"
  else
    git remote add origin "$GIT_REMOTE"
  fi
  say "→ Pushing to $GIT_REMOTE"
  git push -u origin main --force
)

# ===== Summary / DNS Cheatsheet =====
say "✅ DONE. What happened:"
say "1) Created $CODE_TAR and $ASSET_TAR"
say "2) Uploaded to R2 bucket s3://$S3_BUCKET at $S3_ENDPOINT"
say "3) Pushed CLEAN code to GitHub remote: $GIT_REMOTE"
say ""
say "Next steps:"
say "• Vercel is already connected to GitHub -> it will auto-deploy from 'main'."
say "• Durable landing page: in Durable dashboard, copy the CNAME target they show, then set DNS:"
say "    - If you want ROOT on Durable: set an ALIAS/ANAME @ -> <durable-target> (or follow Durable's root instructions)"
say "      and set CNAME www -> <durable-target> (Durable docs show the exact target)."
say "    - If you want APP on Vercel: set CNAME ${APP_SUBDOMAIN} -> cname.vercel-dns.com"
say "      (Alternative for root on Vercel: A @ -> 76.76.21.21; CNAME www -> cname.vercel-dns.com)"
say ""
say "• Assets in R2:"
say "    - Keep private for backups, or make public and later map ${ASSETS_PREFIX}.${ROOT_DOMAIN} to the bucket via Cloudflare."
say ""
say "Verify R2 upload now with:"
say "  aws s3 ls s3://$S3_BUCKET/ --endpoint-url $S3_ENDPOINT"

Make it executable and run:

chmod +x efh_migrate.sh
./efh_migrate.sh

That’s it. It will:

build code-only and assets-only archives,

upload both to R2,

unpack the code-only archive into a clean temp folder,

generate a safe .gitignore,

push to GitHub (the repo you set in GIT_REMOTE).


Vercel (already connected) will redeploy from main.


---

DNS quick guide (fill in when ready)

Pick one of these layouts:

A) Durable at root, Vercel at app

@ (root) → use the CNAME/ALIAS Durable gives you (Durable’s dashboard shows the exact target; some registrars require ALIAS/ANAME for root).

www → CNAME → Durable target (so www goes to Durable too)

app → CNAME → cname.vercel-dns.com


B) Vercel at root, Durable at www (or a subdomain)

@ → A → 76.76.21.21

www → CNAME → Durable target (or also to Vercel if you prefer)

(optional) assets → when you later set R2 public and create a custom domain, point assets to the R2 worker/endpoint you configure in Cloudflare.


> Durable and Vercel both show the exact DNS targets in their dashboards; use those if they differ from the common values above.




---

Verify uploads / repo

R2 listing:

aws s3 ls s3://efh-archive/ --endpoint-url https://<ACCOUNT_ID>.r2.cloudflarestorage.com

GitHub: open your repo URL and confirm files look clean (no node_modules, no giant asset folders).


---

If you want the script to also set your Vercel domains automatically from Replit (and print the needed records), I can add the Vercel-CLI section too—just share your VERCEL_TOKEN and which domain you want as primary (root vs www/app).

