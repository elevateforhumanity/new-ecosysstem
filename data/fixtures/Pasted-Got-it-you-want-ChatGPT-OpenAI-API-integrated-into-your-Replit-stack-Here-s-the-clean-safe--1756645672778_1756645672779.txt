Got it—you want **ChatGPT (OpenAI API) integrated into your Replit stack**.
Here’s the clean, safe way to do it in your **Brain/Pay Node.js Repl** plus a tiny snippet you can call from any of your front-end sites.

---

# 1) Add OpenAI to your Brain (Node.js Repl)

**a) Install**

```bash
npm i openai
```

**b) Secret in Replit**

* Add a secret named **`OPENAI_API_KEY`** with your OpenAI key.

**c) Minimal server route (`src/routes/ai.js`)**

```js
// src/routes/ai.js
import { Router } from "express";
import OpenAI from "openai";

export const ai = Router();
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/**
 * POST /api/ai/chat
 * body: { messages: [{role:'user'|'system'|'assistant', content:string}], model?:string }
 */
ai.post("/api/ai/chat", async (req, res) => {
  try {
    const { messages = [], model = "gpt-4o-mini" } = req.body || {};
    if (!Array.isArray(messages) || !messages.length) {
      return res.status(400).json({ error: "messages[] required" });
    }

    const completion = await client.chat.completions.create({
      model,                       // fast & affordable. Use "gpt-4o" for higher quality
      messages,
      temperature: 0.3
    });

    const text = completion.choices?.[0]?.message?.content ?? "";
    res.json({ text, raw: completion });
  } catch (e) {
    console.error("AI error:", e?.message);
    res.status(500).json({ error: "AI request failed" });
  }
});
```

**d) Wire the route in your server**

```js
// src/server.js
import { ai } from "./routes/ai.js";
app.use(ai);
```

> Why backend? Your OpenAI key stays **secret**. Never put it in frontend code.

**Docs:** Official OpenAI model and SDK guidance (Node/JS, models like GPT-4o) are here: ([OpenAI Platform][1])

---

# 2) Optional: Streaming responses (typing effect)

If you want streaming to the browser, add this alternate endpoint:

```js
// src/routes/ai-stream.js
import { Router } from "express";
import OpenAI from "openai";

export const aiStream = Router();
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

aiStream.post("/api/ai/stream", async (req, res) => {
  try {
    const { messages = [], model = "gpt-4o-mini" } = req.body || {};

    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");

    const stream = await client.chat.completions.create({
      model,
      messages,
      temperature: 0.3,
      stream: true
    });

    for await (const chunk of stream) {
      const delta = chunk.choices?.[0]?.delta?.content;
      if (delta) res.write(`data: ${JSON.stringify({ delta })}\n\n`);
    }
    res.write(`data: ${JSON.stringify({ done: true })}\n\n`);
    res.end();
  } catch (e) {
    res.write(`data: ${JSON.stringify({ error: "stream failed" })}\n\n`);
    res.end();
  }
});
```

…and register it:

```js
import { aiStream } from "./routes/ai-stream.js";
app.use(aiStream);
```

---

# 3) Frontend snippet (any sister site)

Call your brain’s AI route from the browser (no secrets exposed):

```html
<script>
async function askAI(prompt) {
  const res = await fetch("https://efh-pay.<your-user>.repl.co/api/ai/chat", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      messages: [
        { role: "system", content: "You are Elevate for Humanity's helpful assistant." },
        { role: "user", content: prompt }
      ]
    })
  });
  const data = await res.json();
  if (data.error) throw new Error(data.error);
  return data.text;
}

// Example usage: wire to a button
async function onAskClick() {
  const q = document.getElementById("q").value;
  document.getElementById("out").textContent = "Thinking…";
  try {
    const a = await askAI(q);
    document.getElementById("out").textContent = a;
  } catch (e) {
    document.getElementById("out").textContent = "Error: " + e.message;
  }
}
</script>

<input id="q" placeholder="Ask the EFH Assistant…"/>
<button onclick="onAskClick()">Ask</button>
<pre id="out"></pre>
```

For **streaming**, connect to `/api/ai/stream` with an `EventSource` or `fetch` + reader, and append `delta` chunks to the UI.

---

# 4) Guardrails & costs

* Set a **request size limit** (already using `express.json({ limit: '1mb' })`).
* Add **rate limiting** (already present in your server template).
* Keep **temperature** low (0–0.4) for reliable, focused responses.
* You can choose models:

  * `gpt-4o-mini` → fast/cheap for general chat.
  * `gpt-4o` → higher quality for complex tasks. ([OpenAI Platform][1])

---

# 5) Where does each piece go?

* **Brain (Node.js Repl):** `openai` install, `OPENAI_API_KEY` secret, `src/routes/ai.js` (and optional `ai-stream.js`), plus `app.use(...)` lines.
* **Frontends (landing/programs/lms/connect):** only the **fetch** snippet to call your brain (`/api/ai/chat` or `/api/ai/stream`). No API keys in the browser.

---

If you want, I can also scaffold a **small “Ask EFH” chat widget** (floating button + drawer, like your Account Drawer) that talks to `/api/ai/chat` and you can drop it on every site.

[1]: https://platform.openai.com/docs/models/gpt-4o?snapshot=gpt-4o-2024-11-20&utm_source=chatgpt.com "Model - OpenAI API"
